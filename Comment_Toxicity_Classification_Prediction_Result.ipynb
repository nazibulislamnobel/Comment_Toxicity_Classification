{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Import Libraries and Load Trained Model***"
      ],
      "metadata": {
        "id": "No9BD75vr0i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "with open('/content/drive/MyDrive/trained_model.pkl', 'rb') as model_file:\n",
        "    clf = pickle.load(model_file)\n",
        "\n",
        "label_english = {0: 'Good', 1: 'Harmful'}\n",
        "label_bengali = {0: 'ক্ষতিকর না', 1: 'ক্ষতিকর'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX8NbABDvfIH",
        "outputId": "564bb1e4-2d61-44ea-ab33-39620f8b50f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Define Tokenizer Functions***"
      ],
      "metadata": {
        "id": "LR2vin1uvj4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower())\n",
        "    text += ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in tokenizer_porter(text) if w not in stop]\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "0aDVtqyxvlkB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vectorization for Both***"
      ],
      "metadata": {
        "id": "rxpyh17ZvoET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "5l5I32cRv9Rs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Prediction in English***"
      ],
      "metadata": {
        "id": "n0D1Dkyxr77Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_english = [\"Go to Hell. Fuck you\"]\n",
        "X_english = vect.transform(example_english)\n",
        "\n",
        "prediction_english = label_english[clf.predict(X_english)[0]]\n",
        "probability_english = np.max(clf.predict_proba(X_english)) * 100\n",
        "\n",
        "print('English Prediction:')\n",
        "print(f'Prediction: {prediction_english}\\nProbability: {probability_english:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUs-rWV9vrZD",
        "outputId": "bc6d2371-ac87-432e-e37d-c3aee2ccac45"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Prediction:\n",
            "Prediction: Harmful\n",
            "Probability: 99.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Prediction in Bengali***"
      ],
      "metadata": {
        "id": "0Pmcc7NhsFrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_bengali = [\"একটু হিসাব করে দেখেন, আর্জেন্টিনার জনসংখ্যা ৪ কোটি... আর আমাদের ১৬ কোটির দেশে প্রায় অর্ধেকই (৮ কোটি) হলো আর্জেন্টিনার সাপোর্টার\"]\n",
        "X_bengali = vect.transform(example_bengali)\n",
        "\n",
        "prediction_bengali = label_bengali[clf.predict(X_bengali)[0]]\n",
        "probability_bengali = np.max(clf.predict_proba(X_bengali)) * 100\n",
        "\n",
        "print('\\nBengali Prediction:')\n",
        "print(f'Prediction: {prediction_bengali}\\nProbability: {probability_bengali:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysgUSXAXv1Mk",
        "outputId": "6a9e81aa-33c4-4967-bff9-5fefc3af1db8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bengali Prediction:\n",
            "Prediction: ক্ষতিকর না\n",
            "Probability: 97.38%\n"
          ]
        }
      ]
    }
  ]
}
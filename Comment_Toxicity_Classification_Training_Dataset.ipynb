{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Use this cell only when you use google colab***"
      ],
      "metadata": {
        "id": "hrGcFGHtqpZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Cq-wDRYMmWIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Import Libraries and Download NLTK Data***"
      ],
      "metadata": {
        "id": "sarB4Dxbpz2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "U5cwMvuXowmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Define Preprocessing Function***"
      ],
      "metadata": {
        "id": "pvQdVgj7p30I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_socialmediacomments(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub('<[^>]*>', '', text)\n",
        "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "        text = re.sub('[\\W]+', ' ', text.lower())\n",
        "        text = text + ' '.join(emoticons).replace('-', '')\n",
        "        return text\n",
        "    else:\n",
        "        return ''"
      ],
      "metadata": {
        "id": "g4fUvRXsp3GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load Dataset and Apply Preprocessing***"
      ],
      "metadata": {
        "id": "Mw98WQfzqAIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Test Bangla & English Harmful Comment Dataset.xlsx')\n",
        "df['Comment'] = df['Comment'].progress_apply(preprocess_socialmediacomments)"
      ],
      "metadata": {
        "id": "R_6alL3_qFWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization and Vectorization***"
      ],
      "metadata": {
        "id": "b05HPi7GqJph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower())\n",
        "    text += ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in tokenizer_porter(text) if w not in stop]\n",
        "    return tokenized\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "HxOUh49qqPdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train the Model and Save***"
      ],
      "metadata": {
        "id": "L29RRx7Hqt05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"Comment\"].to_list()\n",
        "y = df['Harmful']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "X_train = vect.transform(X_train)\n",
        "X_test = vect.transform(X_test)\n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1)\n",
        "classes = np.array([0, 1])\n",
        "\n",
        "clf.partial_fit(X_train, y_train, classes=classes)\n",
        "\n",
        "# Save the trained model using pickle in your specified path\n",
        "save_path = '/your/specified/path/trained_model.pkl'\n",
        "with open(save_path, 'wb') as model_file:\n",
        "    pickle.dump(clf, model_file)\n",
        "\n",
        "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "bMsVKHaZqw7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}